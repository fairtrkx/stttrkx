{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Building True Edges_\n",
    "\n",
    "1. _~~layerwise true edges~~_\n",
    "2. _~~modulewise true edges~~_\n",
    "3. _orderwise true edges (new for curly tracks)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, sys, yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import seaborn as sns\n",
    "import trackml.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append parent dir\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cuda gpus if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "from src import SttCSVDataReader, SttTorchDataReader\n",
    "from src import detector_layout\n",
    "from src import Build_Event, Build_Event_Viz, Visualize_Edges\n",
    "from src.math_utils import polar_to_cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Input Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "input_dir = \"../data_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find All Input Data Files (hits.csv, cells.csv, particles.csv, truth.csv)\n",
    "all_files = os.listdir(input_dir)\n",
    "\n",
    "# Extract File Prefixes (use e.g. xxx-hits.csv)\n",
    "suffix = \"-hits.csv\"\n",
    "file_prefixes = sorted(\n",
    "    os.path.join(input_dir, f.replace(suffix, \"\"))\n",
    "    for f in all_files\n",
    "    if f.endswith(suffix)\n",
    ")\n",
    "\n",
    "print(\"Number of Files: \", len(file_prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_prefixes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an event\n",
    "# hits, tubes, particles, truth = trackml.dataset.load_event(file_prefixes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits.head()\n",
    "# tubes.head()\n",
    "# particles.head()\n",
    "# truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _Visualize Event_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select event\n",
    "event_id = 95191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose event is exactly the same as select_hits()\n",
    "# event = Build_Event(input_dir, event_id, noise=False, skewed=False, selection=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize event\n",
    "# Build_Event_Viz(event, figsize=(10,10), fig_type=\"pdf\", save_fig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _3. Orderwise True Edges_\n",
    "\n",
    "Ground truth constructed from _`get_layerwise_edges()`_ and _`get_modulewise_edges()`_ heuristics works best for high $p_t$ particle trajectories _i.e._ ones that leave the detector. However, both of these methods fails when a low $p_t$ particle either re-enters the detector or simply spiral inside the detector. One needs a new heuristic for such tracks. Instead of sorting hits according to $R = \\sqrt(x^2 + y^2 + z^2)$, one needs something that follows the praticle trajectory to create the grouth truth. There different ideas who to achieve that:\n",
    "\n",
    "- By using a time parameter to sort hits in a track and connecting in order. Effectively hits will get connected along the path. Although timing is available during simulation but it get disturbed during digitization.\n",
    "- Although timing parmeter can not be used but we can assume there is inherent order, after all, hits are produced along the track sequentially during the simulation. One needs to make sure, any manipulation on the data later one does not disturb the order.\n",
    "\n",
    "Another way is to sort the hits based on $\\phi$ and $\\theta$ in addition to $R$. In a magnetic field, a particle either bends left of right based on its charge. In principle, one can sort hits using the $\\phi$ in addition to $R$. Alternatively, one make use of direction cosines to sort hits along the particle trajectory.\n",
    "\n",
    "Lastly, one can connect hits by using some sort of Nearest-Neighbor method.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _3.1 - Order of Occurence of Hits [Success]_\n",
    "\n",
    "We assume there is an inherent order in the hit position of a particle trajectory. This is confimed when looking into the timing of hits during _simulation_. So in principle one can use timing to keep track the order. However, timing might not be useful after digitization but it should not disturbed the order. Now we check while processing an event this order is not disturbed by different operations on the DataFrames. We with investigate `select_hits()` where a lot of processing is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_particles(particles, selection=False):\n",
    "    \"\"\"Special manipulation on particles dataframe\"\"\"\n",
    "\n",
    "    # find nhits, and drop_duplicates\n",
    "    particles[\"nhits\"] = particles.groupby([\"particle_id\"])[\"nhits\"].transform(\"count\")\n",
    "    particles.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "    if selection:\n",
    "        # just keep protons, pions, don't forget resetting index and dropping old one.\n",
    "        particles = particles[\n",
    "            particles[\"pdgcode\"].isin([-2212, 2212, -211, 211])\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_hits(event_file=None, noise=False, skewed=False, **kwargs):\n",
    "    \"\"\"Hit selection method from Exa.TrkX. Build a full event, select hits based on certain criteria.\"\"\"\n",
    "\n",
    "    # load data using event_prefix (e.g. path/to/event0000000001)\n",
    "    hits, tubes, particles, truth = trackml.dataset.load_event(event_file)\n",
    "\n",
    "    # FIXME: Add an index column to preserve the original order of hits\n",
    "    hits[\"original_order\"] = hits.index\n",
    "\n",
    "    # preprocess particles dataframe e.g. nhits, drop_duplicates, etc.\n",
    "    particles = process_particles(particles, selection=kwargs[\"selection\"])\n",
    "\n",
    "    # skip noise hits.\n",
    "    if noise:\n",
    "        # runs if noise=True\n",
    "        truth = truth.merge(\n",
    "            # particles[[\"particle_id\", \"vx\", \"vy\", \"vz\"]], on=\"particle_id\", how=\"left\"\n",
    "            particles[[\"particle_id\", \"vx\", \"vy\", \"vz\", \"q\", \"pdgcode\"]],\n",
    "            on=\"particle_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "    else:\n",
    "        # runs if noise=False\n",
    "        truth = truth.merge(\n",
    "            # particles[[\"particle_id\", \"vx\", \"vy\", \"vz\"]], on=\"particle_id\", how=\"inner\"\n",
    "            particles[[\"particle_id\", \"vx\", \"vy\", \"vz\", \"q\", \"pdgcode\"]],\n",
    "            on=\"particle_id\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "\n",
    "    # derive new quantities from truth\n",
    "    px = truth.tpx\n",
    "    py = truth.tpy\n",
    "    pz = truth.tpz\n",
    "\n",
    "    # calculate pt, ptheta, peta, pphi\n",
    "    pt = np.sqrt(px**2 + py**2)\n",
    "    ptheta = np.arctan2(pt, pz)  # OR, np.arccos(pz/p)\n",
    "    peta = -np.log(np.tan(0.5 * ptheta))\n",
    "    pphi = np.arctan2(py, px)\n",
    "\n",
    "    # assign pt, ptheta, peta, pphi to truth\n",
    "    truth = truth.assign(pt=pt, ptheta=ptheta, peta=peta, pphi=pphi)\n",
    "\n",
    "    # FIXME: Check if Order is Changed\n",
    "    assert (\n",
    "        hits[\"original_order\"] == hits.index\n",
    "    ).all(), \"Order disturbed after merging with tubes\"\n",
    "\n",
    "    # merge some columns of tubes to the hits, I need isochrone, skewed & sector_id\n",
    "    hits = hits.merge(\n",
    "        tubes[[\"hit_id\", \"isochrone\", \"skewed\", \"sector_id\"]], on=\"hit_id\"\n",
    "    )\n",
    "\n",
    "    # FIXME: Check if Order is Changed\n",
    "    assert (\n",
    "        hits[\"original_order\"] == hits.index\n",
    "    ).all(), \"Order disturbed after merging with tubes\"\n",
    "\n",
    "    # skip skewed tubes\n",
    "    if skewed is False:\n",
    "        print(\"Removing Skewed Layers...\")\n",
    "        # filter non-skewed layers (skewed==0 for non-skewed layers & skewed==1 for skewed layers)\n",
    "        hits = hits.query(\"skewed==0\")\n",
    "\n",
    "        # rename layer_ids from 0,1,2...,17 & assign a new colmn named \"layer\"\n",
    "        vlids = hits.layer_id.unique()\n",
    "        n_det_layers = len(vlids)\n",
    "        vlid_groups = hits.groupby([\"layer_id\"])\n",
    "        hits = pd.concat(\n",
    "            [\n",
    "                vlid_groups.get_group(vlids[i]).assign(layer_id=i)\n",
    "                for i in range(n_det_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # FIXME: Check if Order is Changed\n",
    "    assert (\n",
    "        hits[\"original_order\"] == hits.index\n",
    "    ).all(), \"Order disturbed after removing skewed tubes\"\n",
    "\n",
    "    # Calculate derived variables from 'hits'\n",
    "    r = np.sqrt(hits.x**2 + hits.y**2)\n",
    "    phi = np.arctan2(hits.y, hits.x)\n",
    "    r3 = np.sqrt(hits.x**2 + hits.y**2 + hits.z**2)\n",
    "    theta = np.arccos(hits.z / r3)\n",
    "    eta = -np.log(np.tan(theta / 2.0))\n",
    "\n",
    "    # Merge 'hits' with 'truth', but first add r, phi, theta, eta\n",
    "    hits = hits.assign(r=r, phi=phi, theta=theta, eta=eta).merge(truth, on=\"hit_id\")\n",
    "\n",
    "    # FIXME: Check if Order is Changed\n",
    "    # assert (hits['original_order'] == hits.index).all(), \"Order disturbed after merging with truth\"\n",
    "\n",
    "    # FIXME: Restore the original order of hits using the 'original_order' column\n",
    "    hits = hits.sort_values(by=\"original_order\").reset_index(drop=True)\n",
    "\n",
    "    # Add 'event_id' column to this event.\n",
    "    hits = hits.assign(event_id=int(event_file[-10:]))\n",
    "\n",
    "    # FIXME: Drop the original_order column as it is no longer needed\n",
    "    hits = hits.drop(columns=[\"original_order\"])\n",
    "\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _1st Attempt_\n",
    "\n",
    "- assuming the hits are in the order of occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orderwise_edges(hits, column=\"hit_id\"):\n",
    "    \"\"\"The function closely resembles get_modulewise_edges(), one\n",
    "    can introduce layerwise variant similar to get_layerwise_edges\"\"\"\n",
    "\n",
    "    # Group by particle_id, similar to modulewise edges\n",
    "    groups = hits.groupby([\"particle_id\"])\n",
    "\n",
    "    # Create an empty list to store the edge arrays for each group\n",
    "    true_edges = []\n",
    "\n",
    "    # Iterate over each group\n",
    "    for _, group in groups:\n",
    "\n",
    "        # Use 'hit_id' column to create true_edges, I assume order\n",
    "        # of occurence of hits is along the particle trajectory.\n",
    "        # hit_indices = group['hit_id'].values\n",
    "\n",
    "        # Or, use index of hits to create true_edges, I assume order\n",
    "        # of occurence of hits is along the particle trajectory [KEEP it].\n",
    "        hit_indices = group.index.values\n",
    "\n",
    "        # Create arrays for source and target nodes\n",
    "        source_nodes = hit_indices[:-1]\n",
    "        target_nodes = hit_indices[1:]\n",
    "\n",
    "        # Concatenate the source and target arrays vertically\n",
    "        edge_array = np.column_stack((source_nodes, target_nodes))\n",
    "\n",
    "        # Append the edge array to the list\n",
    "        true_edges.append(edge_array)\n",
    "\n",
    "    # Concatenate for all particle groups vertically\n",
    "    true_edges = np.vstack(true_edges)\n",
    "    return true_edges.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get event prefix using event_id\n",
    "event_prefix = file_prefixes[event_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select hits\n",
    "kwargs = {\"selection\": False}\n",
    "hits = select_hits(event_file=event_prefix, noise=False, skewed=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true edges\n",
    "true_edges = get_orderwise_edges(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new (1): plotting true edges\n",
    "\n",
    "# detector layout\n",
    "fig, ax = detector_layout(figsize=(10, 10))\n",
    "\n",
    "# particle tracks\n",
    "pids = np.unique(hits.particle_id)\n",
    "for pid in pids:\n",
    "    idx = hits.particle_id == pid\n",
    "    ax.scatter(hits[idx].x.values, hits[idx].y.values, label=\"particle_id: %d\" % pid)\n",
    "\n",
    "# loop over source and target nodes\n",
    "# for i, (source_node, target_node) in enumerate(true_edges.T):\n",
    "for source_node, target_node in true_edges.T:\n",
    "    source_pos = hits.loc[source_node, [\"x\", \"y\"]].values\n",
    "    target_pos = hits.loc[target_node, [\"x\", \"y\"]].values\n",
    "    ax.plot(\n",
    "        [source_pos[0], target_pos[0]],\n",
    "        [source_pos[1], target_pos[1]],\n",
    "        \"k-\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "# axis params\n",
    "ax.legend(fontsize=12, loc=\"best\")\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"orderwise_true_edges.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _2nd Attempt_\n",
    "\n",
    "- following the logic of _`get_modulewise_edges()`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modulewise_edges_ordered(hits):\n",
    "    \"\"\"Get modulewise (layerless) true edge list using the order\n",
    "    of occurence hits. Here 'hits' represent complete event.\"\"\"\n",
    "\n",
    "    # Handle NaN and Null Values\n",
    "    signal = hits[\n",
    "        ((~hits.particle_id.isna()) & (hits.particle_id != 0)) & (~hits.vx.isna())\n",
    "    ]\n",
    "    signal = signal.drop_duplicates(\n",
    "        subset=[\"particle_id\", \"volume_id\", \"layer_id\", \"module_id\"]\n",
    "    )\n",
    "\n",
    "    # Reset index to get 'index' column\n",
    "    signal = signal.reset_index(drop=False)\n",
    "\n",
    "    # Preserve the Order\n",
    "    # signal = signal.rename(columns={\"index\": \"unsorted_index\"}).reset_index(drop=False)\n",
    "\n",
    "    # Handle Particle_id 0\n",
    "    signal.loc[signal[\"particle_id\"] == 0, \"particle_id\"] = np.nan\n",
    "\n",
    "    # Group by particle ID and get list of indices of every particle (series of series).\n",
    "    signal_list = signal.groupby([\"particle_id\"], sort=False)[\"index\"].agg(\n",
    "        lambda x: list(x)\n",
    "    )\n",
    "\n",
    "    # Generate Edges\n",
    "    true_edges = []\n",
    "    for row in signal_list.values:\n",
    "        for i, j in zip(row[:-1], row[1:]):\n",
    "            true_edges.append([i, j])\n",
    "\n",
    "    # Return Edges\n",
    "    true_edges = np.array(true_edges).T\n",
    "\n",
    "    # Restore the Order\n",
    "    # true_edges = signal.unsorted_index.values[true_edges]\n",
    "\n",
    "    return true_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select hits\n",
    "kwargs = {\"selection\": False}\n",
    "hits = select_hits(event_file=event_prefix, noise=False, skewed=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true edges\n",
    "true_edges = get_modulewise_edges_ordered(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new (2): plotting true edges\n",
    "\n",
    "# detector layout\n",
    "fig, ax = detector_layout(figsize=(10, 10))\n",
    "\n",
    "# particle tracks\n",
    "hits_grouped = hits.groupby(\"particle_id\")\n",
    "for particle_id, group in hits_grouped:\n",
    "    ax.scatter(group[\"x\"], group[\"y\"], label=f\"particle_id: {particle_id}\")\n",
    "\n",
    "# loop over source and target nodes\n",
    "# for i, (source_node, target_node) in enumerate(true_edges.T):\n",
    "for source_node, target_node in true_edges.T:\n",
    "    source_pos = hits.loc[source_node, [\"x\", \"y\"]].values\n",
    "    target_pos = hits.loc[target_node, [\"x\", \"y\"]].values\n",
    "    ax.plot(\n",
    "        [source_pos[0], target_pos[0]],\n",
    "        [source_pos[1], target_pos[1]],\n",
    "        \"k-\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "# axis params\n",
    "ax.legend(fontsize=12, loc=\"best\")\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"modulewise_edges_ordered.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _3rd Attempt [Failed]_\n",
    "\n",
    "- following the logic of _`get_layerwise_edges()`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layerwise_edges_ordered(hits):\n",
    "    \"\"\"Get modulewise (layerless) true edge list using the order\n",
    "    of occurence hits. Here 'hits' represent complete event.\"\"\"\n",
    "\n",
    "    hits = hits.reset_index()\n",
    "\n",
    "    hits.loc[hits[\"particle_id\"] == 0, \"particle_id\"] = np.nan\n",
    "    hit_list = (\n",
    "        hits.groupby([\"particle_id\", \"layer_id\"], sort=False)[\n",
    "            \"index\"\n",
    "        ]  # ADAK: layer >> layer_id\n",
    "        .agg(lambda x: list(x))\n",
    "        .groupby(level=0)\n",
    "        .agg(lambda x: list(x))\n",
    "    )\n",
    "\n",
    "    true_edges = []\n",
    "    for row in hit_list.values:\n",
    "        for i, j in zip(row[0:-1], row[1:]):\n",
    "            true_edges.extend(list(itertools.product(i, j)))\n",
    "\n",
    "    true_edges = np.array(true_edges).T\n",
    "    return true_edges, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select hits\n",
    "kwargs = {\"selection\": False}\n",
    "hits = select_hits(event_file=event_prefix, noise=False, skewed=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true edges\n",
    "true_edges, hit = get_layerwise_edges_ordered(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new (2): plotting true edges\n",
    "\n",
    "# detector layout\n",
    "fig, ax = detector_layout(figsize=(10, 10))\n",
    "\n",
    "# particle tracks\n",
    "hits_grouped = hits.groupby(\"particle_id\")\n",
    "for particle_id, group in hits_grouped:\n",
    "    ax.scatter(group[\"x\"], group[\"y\"], label=f\"particle_id: {particle_id}\")\n",
    "\n",
    "# loop over source and target nodes\n",
    "# for i, (source_node, target_node) in enumerate(true_edges.T):\n",
    "for source_node, target_node in true_edges.T:\n",
    "    source_pos = hits.loc[source_node, [\"x\", \"y\"]].values\n",
    "    target_pos = hits.loc[target_node, [\"x\", \"y\"]].values\n",
    "    ax.plot(\n",
    "        [source_pos[0], target_pos[0]],\n",
    "        [source_pos[1], target_pos[1]],\n",
    "        \"k-\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "# axis params\n",
    "ax.legend(fontsize=12, loc=\"best\")\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"layerwise_edges_ordered.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "### _3.2 - Angle-based Sorting of Hits [Failed]_\n",
    "\n",
    "There are several reasons why this methods fails _e.g._ two hits in the same layer, uncertanity in the hit position as successive layers have hits that are not aligned within the precision of $\\phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modulewise_edges_with_angles(hits):\n",
    "    \"\"\"Get modulewise (layerless) true edge list, considering angular sorting.\"\"\"\n",
    "    # Filter out the hits that correspond to actual particles\n",
    "    signal = hits[\n",
    "        ((~hits.particle_id.isna()) & (hits.particle_id != 0)) & (~hits.vx.isna())\n",
    "    ]\n",
    "    signal = signal.drop_duplicates(\n",
    "        subset=[\"particle_id\", \"volume_id\", \"layer_id\", \"module_id\"]\n",
    "    )\n",
    "\n",
    "    # Compute the radial distance R and angles phi and theta\n",
    "    signal = signal.assign(\n",
    "        R=np.sqrt(\n",
    "            (signal.x - signal.vx) ** 2\n",
    "            + (signal.y - signal.vy) ** 2\n",
    "            + (signal.z - signal.vz) ** 2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Group by particle ID\n",
    "    signal_list = signal.groupby(\"particle_id\", sort=False)\n",
    "\n",
    "    true_edges = []\n",
    "    for pid, group in signal_list:\n",
    "        # Sort by R and then by phi, theta to handle re-entry\n",
    "        group = group.sort_values([\"R\", \"pphi\", \"ptheta\"]).reset_index(drop=False)\n",
    "\n",
    "        # Handle re-indexing\n",
    "        group = group.rename(columns={\"index\": \"unsorted_index\"}).reset_index(\n",
    "            drop=False\n",
    "        )\n",
    "\n",
    "        row_indices = group.index.values\n",
    "        for i, j in zip(row_indices[:-1], row_indices[1:]):\n",
    "            true_edges.append(\n",
    "                [group.loc[i, \"unsorted_index\"], group.loc[j, \"unsorted_index\"]]\n",
    "            )\n",
    "\n",
    "    true_edges = np.array(true_edges).T\n",
    "    return true_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select hits\n",
    "kwargs = {\"selection\": False}\n",
    "hits = select_hits(event_file=event_prefix, noise=False, skewed=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_edges = get_modulewise_edges_with_angles(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new (2): plotting true edges\n",
    "\n",
    "# detector layout\n",
    "fig, ax = detector_layout(figsize=(10, 10))\n",
    "\n",
    "# particle tracks\n",
    "hits_grouped = hits.groupby(\"particle_id\")\n",
    "for particle_id, group in hits_grouped:\n",
    "    ax.scatter(group[\"x\"], group[\"y\"], label=f\"particle_id: {particle_id}\")\n",
    "\n",
    "# loop over source and target nodes\n",
    "# for i, (source_node, target_node) in enumerate(true_edges.T):\n",
    "for source_node, target_node in true_edges.T:\n",
    "    source_pos = hits.loc[source_node, [\"x\", \"y\"]].values\n",
    "    target_pos = hits.loc[target_node, [\"x\", \"y\"]].values\n",
    "    ax.plot(\n",
    "        [source_pos[0], target_pos[0]],\n",
    "        [source_pos[1], target_pos[1]],\n",
    "        \"k-\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "# axis params\n",
    "ax.legend(fontsize=12, loc=\"best\")\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"modulewise_true_edges_angle.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _3.3. Distance-based Sorting of Hits [Failed]_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# select hits\n",
    "kwargs = {\"selection\": False}\n",
    "hits = select_hits(event_file=event_prefix, noise=False, skewed=False, **kwargs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(hits.columns.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# layerwise true edges & new hits dataframe\n",
    "# true_edges, hits = get_layerwise_edges(hits)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# modulewise true edges\n",
    "true_edges = get_modulewise_edges(hits)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Euclidean Distance of Two Hits to get a better true graph\n",
    "def distance (hits, i, j):\n",
    "    \"\"\"Hits dataframe containing all variables. i,j are two hit ids\"\"\"\n",
    "    pt1 = hits.iloc[i]\n",
    "    pt2 = hits.iloc[j]\n",
    "    d = np.sqrt((pt2.x - pt1.x)**2 + (pt2.y - pt1.y)**2)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get one edge\n",
    "e = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "edge = senders[e], receivers[e]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "edge"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# find first node\n",
    "hits.loc[hits['hit_id'] == edge[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# find second node\n",
    "hits.loc[hits['hit_id'] == edge[1]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "distance(hits, edge[0], edge[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get one edge\n",
    "e = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "edge = senders[e], receivers[e]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mask = []\n",
    "for e in range(true_edges.shape[1]):\n",
    "    edge = senders[e], receivers[e]\n",
    "    d = distance(hits, edge[0], edge[1])\n",
    "    if d >= 10:\n",
    "        # print(\"edge: ({},{}), d: {}\".format(edge[0], edge[1], d))\n",
    "        mask.append(False)\n",
    "    else:\n",
    "        mask.append(True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mask = np.array(mask)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.where(mask == False)[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.where(mask == True)[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "senders, receivers = true_edges"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "senders = senders[mask]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "receivers = receivers[mask]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "senders.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "receivers.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "328+137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Distance Method** doesn't work as one might remove an edge from inner layer to outer layers _i.e._ before and after the **skewed** layers gap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
